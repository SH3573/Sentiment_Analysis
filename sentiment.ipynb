{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Homework 3\n",
    "## Sentiment analysis using Neural Networks\n",
    "\n",
    "Total: 50 Points\n",
    "\n",
    "\n",
    "In this homework we will perform sentiment analysis using a few simple Neural Network based architectures.\n",
    "For this problem we use the IMDB Large Movie Review Dataset. The dataset contains 25,000 highly polar movie reviews for both train and test dataset, each with 12,500 positive (greater than equal to 7/10 rating) and 12,500 negative reviews(less than equal to 4/10 rating). \n",
    "\n",
    "Use \"https://keras.io/\" for keras documentation. Please use Python 3. GPU is not required but it will help improve the training speed for each problem.\n",
    "\n",
    "Please save the notebook with your cell outputs. You will not be graded if your outputs are not present below the homework cell. Also note your outputs will be unique since you will be using your the last numbers of your uni as your random seed (In the third cell). Make sure you submit this iPython file, with the saved outputs. The submission format must be 'hw3/hw3.ipynb'. You will not submit any other files. If you do save your model weights, you will not submit them. You will however, make sure your model weights do get saved in the 'weights' folder and can be retrieved from there as well.\n",
    "\n",
    "Please fill your details below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "import random\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, Reshape, Merge, BatchNormalization, TimeDistributed, Lambda, Activation, LSTM, Flatten, Convolution1D, GRU, MaxPooling1D\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "#from keras import initializers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras import optimizers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#we retrieve train and test file names\n",
    "\n",
    "train_dir = \"./aclImdb/train/\"\n",
    "test_dir = \"./aclImdb/test/\"\n",
    "tr_review = [re_filename for re_filename in listdir(train_dir)]\n",
    "te_review = [re_filename for re_filename in listdir(test_dir)]\n",
    "\n",
    "#we initialize the train and test arrays\n",
    "\n",
    "tr_X = []\n",
    "tr_Y = []\n",
    "te_X = []\n",
    "te_Y = []\n",
    "\n",
    "#we arrange the reviews into the train and test arrays \n",
    "\n",
    "for review_file in tr_review:\n",
    "    f_review = open(train_dir+review_file, \"r\")\n",
    "    str_review = f_review.readline()\n",
    "    str_review = \" \".join(str_review.split(' '))\n",
    "    tr_X.append(str_review)\n",
    "    y_truth = int (review_file.split('.')[0].split('_')[1])\n",
    "    if y_truth>=7:\n",
    "        tr_Y.append(1)\n",
    "    else:\n",
    "        tr_Y.append(0)\n",
    "        \n",
    "for review_file in te_review:\n",
    "    f_review = open(test_dir+review_file, \"r\")\n",
    "    str_review = f_review.readline()\n",
    "    str_review = \" \".join(str_review.split(' '))\n",
    "    te_X.append(str_review)\n",
    "    y_truth = int (review_file.split('.')[0].split('_')[1])\n",
    "    if y_truth>=7:\n",
    "        te_Y.append(1)\n",
    "    else:\n",
    "        te_Y.append(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create the validation set from the train set\n",
    "\n",
    "use the last 4 numbers of your uni for the seed value seed to ensure all answers remain unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#replace 2 (SEED) with the last 4 numbers of your Uni\n",
    "#Uni: sh3573\n",
    "SEED = 3573\n",
    "seed_counter = 0\n",
    "while(1):\n",
    "\n",
    "    shuffle_combine = list(zip(tr_X, tr_Y))\n",
    "    random.seed(SEED+seed_counter)\n",
    "    seed_counter+=1\n",
    "    random.shuffle(shuffle_combine)\n",
    "\n",
    "    tr_X, tr_Y = zip(*shuffle_combine)\n",
    "\n",
    "    val_X = tr_X[:5000]\n",
    "    val_Y = tr_Y[:5000]\n",
    "\n",
    "    counter = 0\n",
    "    for label in val_Y:\n",
    "        counter+=label\n",
    "\n",
    "    print (counter)\n",
    "    print (seed_counter)\n",
    "    if(counter>2400 and counter <2600):\n",
    "        tr_X = tr_X[5000:]\n",
    "        tr_Y = tr_Y[5000:]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train review set : 15000\n",
      "Length of Train label set : 15000\n",
      "Length of Validation review set : 5000\n",
      "Length of Validation label set : 5000\n",
      "Length of Test review set : 25000\n",
      "Length of Test label set : 25000\n",
      "*****************************************\n",
      "Some sample Reviews Train sets and their labels\n",
      "The action in this movie beats Sunny bhai in Gadar. Akshay Kumar possess the superpowers of Leonidus in 300, Neo in Matrix along with Spiderman and Su\n",
      "0\n",
      "The first hour or so of the movie was mostly boring to say the least. However it improved afterwards as the Valentine Party commenced. Apart from the \n",
      "1\n",
      "But this movie was a bore. The history part was fine but the musical part was not. Not one song I cared about and no soundtrack to be heard.<br /><br \n",
      "0\n",
      "Pier Paolo Pasolini, or Pee-pee-pee as I prefer to call him (due to his love of showing male genitals), is perhaps THE most overrated European Marxist\n",
      "0\n",
      "I must tell you right up front, I am certainly NOT an authority on Bollywood films and have only seen a handful. However, if you've never seen one, DO\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Length of Train review set : \" + str(len(tr_X)))\n",
    "print(\"Length of Train label set : \" + str(len(tr_Y)))\n",
    "print(\"Length of Validation review set : \" + str(len(val_X)))\n",
    "print(\"Length of Validation label set : \" + str(len(val_Y)))\n",
    "print(\"Length of Test review set : \" + str(len(te_X)))\n",
    "print(\"Length of Test label set : \" + str(len(te_Y)))\n",
    "print(\"*****************************************\")\n",
    "print(\"Some sample Reviews Train sets and their labels\")\n",
    "print(tr_X[0][:150])\n",
    "print(tr_Y[0])\n",
    "print(tr_X[1][:150])\n",
    "print(tr_Y[1])\n",
    "print(tr_X[2][:150])\n",
    "print(tr_Y[2])\n",
    "print(tr_X[3][:150])\n",
    "print(tr_Y[3])\n",
    "print(tr_X[4][:150])\n",
    "print(tr_Y[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we collect all the reviews from train validation and test set to generate \n",
    "texts = []\n",
    "texts += tr_X \n",
    "texts += te_X \n",
    "texts += val_X\n",
    "len(texts)\n",
    "\n",
    "\n",
    "\n",
    "#we clip the sentence length to first 250 words. \n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "#length of vocab, Tokenizer will only use vocab_len most common words\n",
    "vocab_len = 25000\n",
    "\n",
    "#we tokenize the texts and convert all the words to tokens\n",
    "tokenizer = Tokenizer(num_words=vocab_len)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "token_tr_X = tokenizer.texts_to_sequences(tr_X)\n",
    "token_te_X = tokenizer.texts_to_sequences(te_X)\n",
    "token_val_X = tokenizer.texts_to_sequences(val_X)\n",
    "\n",
    "#to ensure all reviews have the same length, we pad the smaller reviews with 0, \n",
    "#and cut the larger reviews to a max length \n",
    "#(we clip from the top, as the end of the reviews generally have a conclusion which provides better features)\n",
    "x_train = sequence.pad_sequences(token_tr_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = sequence.pad_sequences(token_te_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_val = sequence.pad_sequences(token_val_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "#changes the labels to one-hot encoding\n",
    "y_train = np_utils.to_categorical(tr_Y)\n",
    "y_test = np_utils.to_categorical(te_Y)\n",
    "y_val = np_utils.to_categorical(val_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (15000, 250)\n",
      "X_test shape: (25000, 250)\n",
      "X_val shape: (5000, 250)\n",
      "y_train shape: (15000, 2)\n",
      "y_test shape: (25000, 2)\n",
      "y_val shape: (5000, 2)\n",
      "*****************************************\n",
      "Tokenized Reviews Train sets and their labels\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 1.  0.]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.  1.]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 1.  0.]\n",
      "\n",
      "[1161   46  894   22  165  850 9019    8    3 3881   46    6    4  264  161\n",
      " 3562   72  256   16    2]\n",
      "[ 1.  0.]\n",
      "\n",
      "[23082    60    10   111    58    25  2471    58    25    74   636    28\n",
      "     6    34   623    28  4705     8     3   547]\n",
      "[ 1.  0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', x_train.shape)\n",
    "print('X_test shape:', x_test.shape)\n",
    "print('X_val shape:', x_val.shape)\n",
    "\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print('y_val shape:', y_val.shape)\n",
    "\n",
    "\n",
    "print(\"*****************************************\")\n",
    "print(\"Tokenized Reviews Train sets and their labels\")\n",
    "print(x_train[0][:20])\n",
    "print(y_train[0])\n",
    "print()\n",
    "print(x_train[1][:20])\n",
    "print(y_train[1])\n",
    "print()\n",
    "print(x_train[2][:20])\n",
    "print(y_train[2])\n",
    "print()\n",
    "print(x_train[3][:20])\n",
    "print(y_train[3])\n",
    "print()\n",
    "print(x_train[4][:20])\n",
    "print(y_train[4])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********************************************\n",
    "\n",
    "As you can see the reviews have now been transformed into indices to tokenized vocabulary and the labels have been converted to one-hot encoding. We can now go ahead and feed these sequences to Neural Network Models.\n",
    "\n",
    "********************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A\n",
    "\n",
    "Building your first model (5 Points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               6400200   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 402       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,600,602\n",
      "Trainable params: 9,600,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "# Embedding: Dimension=128\n",
    "model.add(Embedding(input_dim=vocab_len, output_dim=128, input_length=MAX_SEQUENCE_LENGTH))\n",
    "# Flatten Embedding\n",
    "model.add(Flatten())\n",
    "# Dense: Dimention = 200\n",
    "model.add(Dense(200))\n",
    "# Activation:Rectilinear\n",
    "model.add(Activation('relu'))\n",
    "# Dense: Dimention = 2\n",
    "model.add(Dense(2))\n",
    "# Activation:Softmax\n",
    "model.add(Activation('softmax'))\n",
    "## compille it here according to instructions\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 59s - loss: 0.3943 - acc: 0.8093 - val_loss: 0.2986 - val_acc: 0.8752\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 58s - loss: 0.0333 - acc: 0.9888 - val_loss: 0.4782 - val_acc: 0.8418\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 58s - loss: 0.0030 - acc: 0.9992 - val_loss: 0.5799 - val_acc: 0.8586\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 58s - loss: 2.3282e-04 - acc: 1.0000 - val_loss: 0.6339 - val_acc: 0.8604\n",
      "Time Used: 0:03:55.007180\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=4,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Time Used: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B\n",
    "\n",
    "Stacking Fully Connected Layers (5 points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               6400200   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 402       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,640,802\n",
      "Trainable params: 9,640,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "# Embedding: Dimension=128\n",
    "model.add(Embedding(input_dim=vocab_len, output_dim=128, input_length=MAX_SEQUENCE_LENGTH))\n",
    "# Flatten Embedding\n",
    "model.add(Flatten())\n",
    "# Dense: Dimention = 200\n",
    "model.add(Dense(200))\n",
    "# Activation:Rectilinear\n",
    "model.add(Activation('relu'))\n",
    "# Dense: Dimention = 200\n",
    "model.add(Dense(200))\n",
    "# Activation:Rectilinear\n",
    "model.add(Activation('relu'))\n",
    "# Dense: Dimention = 2\n",
    "model.add(Dense(2))\n",
    "# Activation:Softmax\n",
    "model.add(Activation('softmax'))\n",
    "## compille it here according to instructions\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 58s - loss: 0.3862 - acc: 0.8143 - val_loss: 0.2911 - val_acc: 0.8756\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 58s - loss: 0.0434 - acc: 0.9856 - val_loss: 0.4406 - val_acc: 0.8532\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 59s - loss: 0.0074 - acc: 0.9976 - val_loss: 0.7522 - val_acc: 0.8536\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 59s - loss: 0.0134 - acc: 0.9957 - val_loss: 0.5872 - val_acc: 0.8478\n",
      "Time Used: 0:03:56.617889\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=4,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Time Used: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C\n",
    "\n",
    "Using LSTMS based networks(5 Points) \n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 3,348,354\n",
      "Trainable params: 3,348,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "# Embedding: Dimension=128\n",
    "model.add(Embedding(input_dim=vocab_len, output_dim=128, input_length=MAX_SEQUENCE_LENGTH))\n",
    "# LSTM: Dimension=128\n",
    "model.add(LSTM(units=128))\n",
    "# Dense: Dimention = 128\n",
    "model.add(Dense(128))\n",
    "# Activation:Rectilinear\n",
    "model.add(Activation('relu'))\n",
    "# Dense: Dimention = 2\n",
    "model.add(Dense(2))\n",
    "# Activation:Softmax\n",
    "model.add(Activation('softmax'))\n",
    "## compille it here according to instructions\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 254s - loss: 0.4422 - acc: 0.7933 - val_loss: 0.3228 - val_acc: 0.8676\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 253s - loss: 0.2416 - acc: 0.9074 - val_loss: 0.3531 - val_acc: 0.8686\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 252s - loss: 0.1445 - acc: 0.9465 - val_loss: 0.3973 - val_acc: 0.8462\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 251s - loss: 0.0777 - acc: 0.9733 - val_loss: 0.4576 - val_acc: 0.8524\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 251s - loss: 0.0587 - acc: 0.9800 - val_loss: 0.6466 - val_acc: 0.8638\n",
      "Time Used: 0:21:04.937462\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Time Used: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D\n",
    "\n",
    "Adding Pretrained Word Embeddings(10 Points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "Correction: The Embedding Layer Dimension (1st box) is 300, not 128.\n",
    "\n",
    "![title](img/model4.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 124252 unique tokens\n",
      "G Word embeddings: 1101020\n",
      "G Null word embeddings: 118498\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "#dimension of Glove Embeddings.\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "#load glove embeddings\n",
    "gembeddings_index = {}\n",
    "with codecs.open('glove.42B.300d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        gembedding = np.asarray(values[1:], dtype='float32')\n",
    "        gembeddings_index[word] = gembedding\n",
    "#\n",
    "f.close()\n",
    "print('G Word embeddings:', len(gembeddings_index))\n",
    "\n",
    "# nb_words contains the total length of vocab\n",
    "nb_words = len(word_index) +1\n",
    "\n",
    "#get glove embeddings for each word in tokenizer.\n",
    "#g_word_embedding_matrix holds the embeddings dictionary\n",
    "g_word_embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    gembedding_vector = gembeddings_index.get(word)\n",
    "    if gembedding_vector is not None:\n",
    "        g_word_embedding_matrix[i] = gembedding_vector\n",
    "        \n",
    "#total words in the tokenizer not in Embedding matrix\n",
    "print('G Null word embeddings: %d' % np.sum(np.sum(g_word_embedding_matrix, axis=1) == 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 250, 300)          37275900  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 37,512,318\n",
      "Trainable params: 37,512,318\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "## to use the glove embeddings, your embedding layer would take the vocab size as input dimension, \n",
    "## Glove embedding dimension as the output dimsion\n",
    "## and you will provide the  embedding dictionary as the 'weights' parameter (!important) to the embedding layer.\n",
    "\n",
    "# Embedding: Dimension=300\n",
    "model.add(Embedding(input_dim=nb_words, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, weights=[g_word_embedding_matrix]))\n",
    "# LSTM: Dimension=128\n",
    "model.add(LSTM(units=128, recurrent_dropout=.2))\n",
    "# Dropout:.2\n",
    "model.add(Dropout(.2))\n",
    "# Dense: Dimention = 128\n",
    "model.add(Dense(128))\n",
    "# Activation:Rectilinear\n",
    "model.add(Activation('relu'))\n",
    "# Dense: Dimention = 2\n",
    "model.add(Dense(2))\n",
    "# Activation:Softmax\n",
    "model.add(Activation('softmax'))\n",
    "## compille it here according to instructions\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 457s - loss: 0.5225 - acc: 0.7366 - val_loss: 0.4722 - val_acc: 0.7728\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 461s - loss: 0.3188 - acc: 0.8655 - val_loss: 0.4005 - val_acc: 0.8198\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 460s - loss: 0.1837 - acc: 0.9313 - val_loss: 0.3668 - val_acc: 0.8736\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 460s - loss: 0.1003 - acc: 0.9674 - val_loss: 0.3746 - val_acc: 0.8758\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 461s - loss: 0.0529 - acc: 0.9828 - val_loss: 0.4458 - val_acc: 0.8736\n",
      "Time Used: 0:38:23.702303\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Time Used: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dont attempt this\n",
    "\n",
    "Stacking LSTM layers\n",
    "\n",
    "Unfortunately it takes very long to train, be aware we can stack LTMSs over each other like this.\n",
    "This requires bottom LSTM to return a sequences instead instead of single vector, which becomes input for the top LSTM.\n",
    "\n",
    "\n",
    "![title](img/model5.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part E\n",
    "\n",
    "Using Convolutional Networks (10 points)\n",
    "\n",
    "Construct the model, shown below. Use the same loss functions and optimizers as before\n",
    "\n",
    "Correction: The Embedding Layer Dimension (1st box) is 300, not 128.\n",
    "\n",
    "![title](img/model6.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 250, 300)          37275900  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 248, 128)          115328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 246, 64)           24640     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 244, 32)           6176      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 7808)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               1999104   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 39,421,662\n",
      "Trainable params: 39,421,662\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "# Embedding: Dimension=300\n",
    "model.add(Embedding(input_dim=nb_words, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, weights=[g_word_embedding_matrix]))\n",
    "# Convolution 1D: Filter=128, Kernel=3\n",
    "model.add(Convolution1D(filters=128, kernel_size=3))\n",
    "# Dropout:.2\n",
    "model.add(Dropout(.2))\n",
    "# Activation:Rectilinear\n",
    "model.add(Activation('relu'))\n",
    "# Convolution 1D: Filter=64, Kernel=3\n",
    "model.add(Convolution1D(filters=64, kernel_size=3))\n",
    "# Dropout:.2\n",
    "model.add(Dropout(.2))\n",
    "# Activation:Rectilinear\n",
    "model.add(Activation('relu'))\n",
    "# Convolution 1D: Filter=32, Kernel=3\n",
    "model.add(Convolution1D(filters=32, kernel_size=3))\n",
    "# Dropout:.2\n",
    "model.add(Dropout(.2))\n",
    "# Activation:Rectilinear\n",
    "model.add(Activation('relu'))\n",
    "# Flatten: \n",
    "model.add(Flatten())\n",
    "# Dense: Dimention = 256\n",
    "model.add(Dense(256))\n",
    "# Dropout:.2\n",
    "model.add(Dropout(.2))\n",
    "# Activation:Rectilinear\n",
    "model.add(Activation('relu'))\n",
    "# Dense: Dimention = 2\n",
    "model.add(Dense(2))\n",
    "# Activation:Softmax\n",
    "model.add(Activation('softmax'))\n",
    "## compille it here according to instructions\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 282s - loss: 0.4132 - acc: 0.7962 - val_loss: 0.2943 - val_acc: 0.8792\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 282s - loss: 0.1636 - acc: 0.9385 - val_loss: 0.3268 - val_acc: 0.8716\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 280s - loss: 0.0801 - acc: 0.9709 - val_loss: 0.4409 - val_acc: 0.8646\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 280s - loss: 0.0440 - acc: 0.9842 - val_loss: 0.5291 - val_acc: 0.8726\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 281s - loss: 0.0331 - acc: 0.9884 - val_loss: 0.6343 - val_acc: 0.8746\n",
      "Time used: 0:23:27.623486\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)\n",
    "\n",
    "end_time =datetime.now()\n",
    "print(\"Time used:\", end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part F\n",
    "\n",
    "Model constructed : (5 points)\n",
    "\n",
    "Test Accuracy Over 87.5%: (5 Points)\n",
    "\n",
    "Bonus: Min(10, Square of (test_score - 88%))\n",
    "\n",
    "Create your best model, use Validation score to judge your best model and check accuracy on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 250, 300)          37275900  \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 246, 128)          192128    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 246, 128)          0         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 246, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 242, 64)           41024     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 242, 64)           0         \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 242, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 37,559,230\n",
      "Trainable params: 37,559,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "# Embedding: Dimension=300\n",
    "model.add(Embedding(input_dim=nb_words, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, weights=[g_word_embedding_matrix]))\n",
    "\n",
    "# Convolution 1D: Filter=128, Kernel=3\n",
    "model.add(Convolution1D(filters=128, kernel_size=5))\n",
    "# Dropout:.2\n",
    "model.add(Dropout(.4))\n",
    "# Activation:Rectilinear\n",
    "model.add(Activation('relu'))\n",
    "# Convolution 1D: Filter=64, Kernel=3\n",
    "model.add(Convolution1D(filters=64, kernel_size=5))\n",
    "# Dropout:.2\n",
    "model.add(Dropout(.4))\n",
    "# Activation:Rectilinear\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(64, recurrent_dropout=.4))\n",
    "model.add(Dropout(.4))\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "# Flatten: \n",
    "#model.add(Flatten())\n",
    "# Dense: Dimention = 256\n",
    "model.add(Dense(256))\n",
    "# Dropout:.2\n",
    "model.add(Dropout(.2))\n",
    "# Activation:Rectilinear\n",
    "model.add(Activation('relu'))\n",
    "# Dense: Dimention = 2\n",
    "model.add(Dense(2))\n",
    "# Activation:Softmax\n",
    "model.add(Activation('softmax'))\n",
    "## compille it here according to instructions\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 487s - loss: 0.5117 - acc: 0.7419 - val_loss: 0.3975 - val_acc: 0.8430\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 485s - loss: 0.3438 - acc: 0.8555 - val_loss: 0.3428 - val_acc: 0.8572\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 486s - loss: 0.2101 - acc: 0.9208 - val_loss: 0.3101 - val_acc: 0.8774\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 489s - loss: 0.1458 - acc: 0.9491 - val_loss: 0.3404 - val_acc: 0.8852\n",
      "Time Used: 0:32:31.598721\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=4,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Time Used: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can keep saving models with different names in model_name, \n",
    "\n",
    "so you can retrieve their weights again for testing, you dont have to retrain \n",
    "(You would have to initialize the model definition again)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plan on using Ensemble averaging, feel free to edit the code below or add multiple models.\n",
    "\n",
    "Make sure they get saved and can be retrieved when executing serially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.07%\n"
     ]
    }
   ],
   "source": [
    "#model.load_weights(bst_model_path)\n",
    "scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part G\n",
    "\n",
    "Explain how Dense, LSTM and Convolution Layers work.\n",
    "\n",
    "Explain Relu, Dropout, and Softmax work.\n",
    "\n",
    "Analyze the architectures you constructed, with the accuracies you achieved and the training time it took. \n",
    "\n",
    "What are some insights you gained with these experiments? \n",
    "\n",
    "(5 Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense Layer: a linear operation in which every input is connected to every output by a weight. Generally followed by a non-linear activation function.\n",
    "\n",
    "LSTM: a special kind of RNN layer that enables support for time series and sequence data in a network and is capable of learning long-term dependencies, which means LSTM is able to connect previous information to the present task. The layer performs additive interactions, which can help improve gradient flow over long sequences during training.\n",
    "\n",
    "Convolution Layer: a linear operation using a subset of the weights of a dense layer. Nearby inputs are connected to nearby outputs. The weights for the convolutions at each location are shared. Due to the weight sharing, and the use of a subset of the weights of a dense layer, there's far less weights than in a dense layer. Generally followed by a non-linear activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu: Relu stands for Rectified linear unit and is a non-linear operation. Usually used after every Convolution operation. Output of Relu is max(0, Input), which implies that it replaces all negative values by 0. The purpose is to introduce non-linearity since most of the real-world data we want to learn would be non-linear.\n",
    "\n",
    "Dropout: Dropout is a regularization technique, which aims to reduce the complexity of the model with the goal to prevent overfitting.\n",
    "\n",
    "Softmax: use in output layer, and can handle classifications with more than two classes. The softmax function squashes the outputs of each unit to be between 0 and 1, just like a sigmoid function. But it also divides each output such that the total sum of the outputs is equal to 1. The output of the softmax function is equivalent to a categorical probability distribution, it tells you the probability that any of the classes are true.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the architectures you constructed, with the accuracies you achieved and the training time it took.Â \n",
    "The best model combined Convolution1D and LSTM together. Layers include glove embedding, Convolution1D(filters=128, kernel_size=5, Dropout(.4), Activation('relu'), Convolution1D(filters=64, kernel_size=5), Dropout(.4), Activation('relu'), LSTM(64, recurrent_dropout=.4), Dropout(.4), Dense(256), Dropout(.2), Activation('relu'), Dense(2), Activation('relu'), compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']). It tooks 32.5 minutes to train and the accuracy rate on test data is 86.07%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights:\n",
    "Adding more layers does not always improve the accuracy of the test dataset, but does tend to overfit. With Dropout( ), the model is less likely to overfit. The trainning time is longer if we increse the kerner size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
